<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ajay Sridhar</title>
  
  <meta name="author" content="Ajay Sridhar">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
  <link rel="icon" href="images/stanford.webp">
</head>
<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ajay Sridhar</name>
              </p>
              <p>
                I am a Computer Science PhD student at Stanford University, co-advised by <a href="https://dorsa.fyi/">Dorsa Sadigh</a> and <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>. My research is supported by the <a href="https://www.nsfgrfp.org/">NSF Graduate Research Fellowship</a>. I am interested in building <a href="https://general-navigation-models.github.io/">generalizable and robust</a> robot learning systems that <a href="https://sites.google.com/view/SACSoN-review">continuously improve</a> with experience.
              </p>
              <p>
                Previously, I was an undergraduate at UC Berkeley, where I worked with <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a> and <a href="https://people.eecs.berkeley.edu/~shah/">Dhruv Shah</a> at the <a href="https://rail.eecs.berkeley.edu/">Robotic AI and Learning Lab</a>.

                I have also worked with <a href="https://web.engr.oregonstate.edu/~tgd/">Thomas Dietterich</a>.
              </p>
              <!-- <p>
                I have been a teaching assistant for <a href="https://inst.eecs.berkeley.edu/~cs188/fa23/">CS 188: Introduction to Artificial Intelligence</a> for the past four semesters under 
                <a href="https://people.eecs.berkeley.edu/~russell/?_ga=2.215437655.1710573181.1701653922-2029608830.1690062065">Prof. Stuart Russell</a>, 
                <a href="https://dawnsong.io/">Prof. Dawn Song</a>, 
                <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/mordatch.html">Dr. Igor Mordatch</a>, 
                and 
                <a href="https://peyrin.github.io/">Peyrin Kao</a>.
              </p> -->
              <p>
                
                <!-- I am interested in the intersection of machine learning and robotics, particularly in the areas of reinforcement learning, imitation learning, and computer vision.
                I am also interested in the applications of these techniques to real-world problems. -->
              <p>
                
                
              </p>
              <p style="text-align:center">
                <a href="mailto:ajaysri@stanford.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=wCRav7EAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/ajaysridhar0">Github</a> &nbsp/&nbsp
                <a href="files/ajay_cv_web.pdf">CV</a> &nbsp/&nbsp
                <a href="https://x.com/ajaysridhar0">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/ajay_web.png" class="hoverZoomLink">
              
              <!-- <a href="images/pfp_mountain 2.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/circle_pfp2.png" class="hoverZoomLink"></a> -->
            </td>
          </tr>
        </tbody>
      </table>
      <!--
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Preprints</heading>
      </td>
    </tr>
  </tbody></table>
      -->

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px 20px 5px 20px;width:100%;vertical-align:middle">
        <heading>Publications</heading>
      </td>
    </tr>
  </tbody></table>

    <!--
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='barx_image' style="width:100%;height:100%;background:#f2f2f2;display:flex;align-items:center;justify-content:center;">
            <span style="font-size:14px;color:#666;">Coming soon</span>
          </div>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://bar-x-anon.github.io/">
          <papertitle>Cross-Embodiment Transfer via Behavior-Aligned Representations</papertitle>
        </a>
        <br>
        <span>Authors: Coming soon</span>
        <br>
        <em>International Conference on Robotics and Automation (ICRA), 2026</em>
        <br>
        <span>Paper: Coming soon</span> /
        <span>Website: Coming soon</span> /
        <span>Video: Coming soon</span>
        <p>Coming soon.</p>
      </td>
    </tr>
  </tbody></table>
  -->

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='memer_image'><video width=100% height=100% muted autoplay loop>
          <source src="images/dusting_viz_compress.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://jen-pan.github.io/memer/">
          <papertitle>MemER: Scaling Up Memory for Robot Control via Experience Retrieval</papertitle>
        </a>
        <br>
        <strong>Ajay Sridhar*</strong>,
        <a href="https://www.linkedin.com/in/jennyypan/">Jennifer Pan*</a>,
        <a href="https://satvik1701.github.io/">Satvik Sharma</a>,
        <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>
        <br>
        <em>International Conference on Learning Representations (ICLR), 2026</em>
        <br>
        <a href="https://arxiv.org/abs/2510.20328">arXiv</a> /
        <a href="https://jen-pan.github.io/memer/">Website</a> /
        <a href="https://www.youtube.com/watch?v=v1NRcz1kAfo">Video</a>
        <p>MemER is a hierarchical policy framework, where the high-level policy is trained to select and track previous task-relevant keyframes from its experience. The high-level policy uses selected keyframes and the most recent frames when generating text instructions for a low-level policy to execute.</p>
      </td>
    </tr>
  </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='lelan_image'><video width=100% height=100% muted autoplay loop>
          <source src="images/lelan3.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://sites.google.com/view/lelan-nav">
          <papertitle>LeLaN: Learning A Language-Conditioned Navigation Policy from In-the-Wild Videos</papertitle>
        </a>
        <br>
        <a href="https://sites.google.com/view/noriaki-hirose/">Noriaki Hirose</a>,
        <a href="https://www.linkedin.com/in/catherineglossop/">Catherine Glossop</a>,
        <strong>Ajay Sridhar</strong>,
        <a href="https://people.eecs.berkeley.edu/~shah/">Dhruv Shah</a>,
        <a href="https://oiermees.github.io/">Oier Mees</a>,
        <a href="http://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
        <br>
        <em>Conference on Robot Learning (CoRL), 2024</em>
        <br>
        <a href="https://arxiv.org/abs/2410.03603">arXiv</a> /
        <a href="https://learning-language-navigation.github.io/">Website</a> /
        <a href="https://github.com/NHirose/learning-language-navigation">Code</a>
        <p>LeLaN is a language-conditioned navigation policy that learns from in-the-wild videos to enable natural language navigation commands for mobile robots.</p>
      </td>
    </tr>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='selfi_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/selfi.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://sites.google.com/view/selfi-rl/">
          <papertitle>SELFI: Autonomous Self-improvement with Reinforcement Learning for Social Navigation</papertitle>
        </a>
        <br>
        <a href="https://sites.google.com/view/noriaki-hirose/">Noriaki Hirose</a>,
        <a href="https://people.eecs.berkeley.edu/~shah/">Dhruv Shah</a>,
        <a href="https://kylesta.ch/">Kyle Stachowicz</a>,
        <strong>Ajay Sridhar</strong>,
        <a href="http://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
        <br>
        <em>Conference on Robot Learning (CoRL), 2024 <font color="black"><strong>(Oral)</strong></font></em>
        <br>
        <a href="https://arxiv.org/abs/2403.00991">arXiv</a> /
        <a href="https://www.youtube.com/watch?v=ElTAc_9a2l4">Summary Video</a> 
        <!-- <a href="https://sites.google.com/view/selfi-rl/dataset">Dataset</a> -->
        <p>SELFI is an online reinforcement learning approach for fine-tuning control policies trained with
          model-based learning. We combine the objective used during model-based learning with a Q-value function
          learned online.</p>
      </td>
    </tr>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='nomad_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/nomad.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://general-navigation-models.github.io/nomad/index.html">
          <papertitle>NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration</papertitle>
        </a>
        <br>
        <strong>Ajay Sridhar</strong>,
        <a href="https://people.eecs.berkeley.edu/~shah/">Dhruv Shah</a>,
        <a href="https://www.linkedin.com/in/catherineglossop/">Catherine Glossop</a>,
        <a href="http://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
        <br>
        <em>ICRA, 2024 <strong style="color:#c50202;">(Best Conference Paper Award)</strong></em>
        <br>
        <em>CoRL 2023 Workshop on Pre-Training for Robot Learning, 2023 <strong>(Oral)</strong></em>
        <br>
        <em>NeurIPS 2023 Workshop on Foundation Models for Decision Making, 2023 <strong>(Oral)</strong></em>
        <br>
        <a href="https://arxiv.org/abs/2310.07896">arXiv</a> /
        <a href="https://www.youtube.com/watch?v=zH8LaIapF6w&ab_channel=RAIL">Summary Video</a> /
        <a href="https://github.com/PrieureDeSion/visualnav-transformer">Code</a>
        <p>
          NoMaD is a novel architecture for robotic navigation in previously unseen environments that uses a unified diffusion policy to jointly represent exploratory task-agnostic behavior and goal-directed task-specific behavior. 
        </p>
      </td>
    </tr>		


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr onmouseout="vint_stop()" onmouseover="vint_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='vint_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/vint_arxiv.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>

        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://general-navigation-models.github.io/vint/index.html">
          <papertitle>ViNT: A Foundation Model for Visual Navigation</papertitle>
        </a>
        <br>
        <a href="https://people.eecs.berkeley.edu/~shah/">Dhruv Shah*</a>,
        <strong>Ajay Sridhar*</strong>,
        <a href="https://dashora7.github.io/">Nitish Dashora*</a>,
        <a href="https://kylesta.ch/">Kyle Stachowicz</a>,
        <a href="https://kevin.black/">Kevin Black</a>,
        <a href="https://sites.google.com/view/noriaki-hirose/">Noriaki Hirose</a>,
        <a href="http://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
        <br>
        <em>Conference on Robot Learning (CoRL), 2023 <strong>(Oral & Live Demonstration)</strong></em>
        <br>
        <em>Bay Area Machine Learning Symposium (BayLearn), 2023 <strong>(Oral)</strong></em>
        <br>
        <a href="https://arxiv.org/abs/2306.14846">arXiv</a> /
        <a href="https://www.youtube.com/watch?v=6kNex5dJ5sQ">Summary Video</a> /
        <a href="https://github.com/PrieureDeSion/visualnav-transformer">Code</a>
        <p>ViNT is a flexible Transformer-based model for visual navigation that can be efficiently adapated to a variety of downstream navigational tasks.</p>
      </td>
    </tr>

    

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='sacson_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/sacson.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://sites.google.com/view/SACSoN-review">
            <papertitle>SACSoN: Scalable Autonomous Control for Social Navigation</papertitle>
          </a>
          <br>
          <a href="https://sites.google.com/view/noriaki-hirose/">Noriaki Hirose</a>,
          <a href="https://people.eecs.berkeley.edu/~shah/">Dhruv Shah</a>,
          <strong>Ajay Sridhar</strong>,
          <a href="http://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
          <br>
          <em>IEEE Robotics and Automation Letters (RA-L), 2023</em>
          <br>
          <em>Conference on Robot Learning (CoRL), 2023 <strong>(Live Demonstration)</strong></em>
          <br>
          <a href="https://arxiv.org/abs/2306.01874">arXiv</a> /
          <a href="https://www.youtube.com/watch?v=AuYwmlUzi28">Summary Video</a> /
          <a href="https://sites.google.com/view/sacson-review/huron-dataset">Dataset</a>
          <p>SACSoN is vision-based navigation policy that learns socially unobtrusive behavior in human-occupied spaces through continual learning.</p>
        </td>
      </tr>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='gnm_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/gnm.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://sites.google.com/view/drive-any-robot">
            <papertitle>GNM: A General Navigation Model to Drive Any Robot</papertitle>
          </a>
          <br>
          <a href="https://people.eecs.berkeley.edu/~shah/">Dhruv Shah*</a>,
          <strong>Ajay Sridhar*</strong>,
          <a href="https://sites.google.com/view/noriaki-hirose/">Noriaki Hirose</a>,
          <a href="http://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
          <br>
          <em>International Conference on Robotics and Automation (ICRA), 2023</em>
          <br>
          <a href="https://arxiv.org/abs/2210.03370">arXiv</a> /
          <a href="https://youtu.be/ICeD6iOglKc">Summary Video</a> /
          <a href="https://github.com/PrieureDeSion/visualnav-transformer">Code</a> /
          <a href="https://www.marktechpost.com/2022/12/22/this-artificial-intelligence-ai-paper-from-uc-berkeley-presents-a-general-navigation-model-gnm-from-an-aggregated-multirobot-dataset-to-drive-any-robot/">Media Coverage</a>
          <p>GNM is vision-based navigation policy trained with a simple goal-reaching objective on a cross-embodiment navigation dataset. 
            It exhibits positive transfer, outperforming specialist models trained on singular embodiment datasets, and generalizes to new robots.
          </p>
        </td>
      </tr>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='exaug_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/exaug.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://sites.google.com/view/exaug-nav">
            <papertitle>ExAug: Robot-Conditioned Navigation Policies via Geometric Experience Augmentation</papertitle>
          </a>
          <br>
          <a href="https://sites.google.com/view/noriaki-hirose/">Noriaki Hirose</a>,
          <a href="https://people.eecs.berkeley.edu/~shah/">Dhruv Shah</a>,
          <strong>Ajay Sridhar</strong>,
          <a href="http://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
          <br>
          <em>International Conference on Robotics and Automation (ICRA), 2023</em>
          <br>
          <a href="https://arxiv.org/abs/2210.07450">arXiv</a> /
          <a href="https://www.youtube.com/watch?v=zWGpaxa1DKY">Summary Video</a> 
          <p>
            ExAug is a vision-based navigation policy that learns to control robots with varying camera types, camera placements, robot sizes, and velocity constraints by applying a novel geometric-aware objective to view augmented data.
          </p>
        </td>
      </tr>

    
    

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/cs188_bot.png" alt="cs188", width="100%">
          </td>
          <td width="75%" valign="center">
            <a href="https://inst.eecs.berkeley.edu/~cs188/sp24/">Undergraduate Student Instructor, CS188 Spring 2024</a>
            <br>
            <a href="https://inst.eecs.berkeley.edu/~cs188/fa23/">Undergraduate Student Instructor, CS188 Fall 2023</a>
            <br>
            <a href="https://inst.eecs.berkeley.edu/~cs188/sp23/">Undergraduate Student Instructor, CS188 Spring 2023</a>
            <br>
            <a href="https://inst.eecs.berkeley.edu/~cs188/fa22/">Undergraduate Student Instructor, CS188 Fall 2022</a>
            <br>
            <a href="https://inst.eecs.berkeley.edu/~cs188/sp22/">Undergraduate Student Instructor, CS188 Spring 2022</a>
          </td>
        </tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/berkeleyEECS.png" alt="berkeleyEECS" width="100%">
          </td>
          <td width="75%" valign="center">
            <a href="https://inst.eecs.berkeley.edu/~eecs16b/fa21/">Tutor, EECS16B Fall 2021</a>
            <br>
          </td>
        </tr>
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Source code from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
