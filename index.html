<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ajay Sridhar</title>
  
  <meta name="author" content="Ajay Sridhar">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
  <link rel="icon" href="images/berkeley.png">
</head>
<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ajay Sridhar</name>
              </p>
              <p>
                I am an undergraduate at UC Berkeley, majoring <a href="https://eecs.berkeley.edu/">Electrical Engineering and Computer Science (EECS)</a> and minoring in <a href="https://guide.berkeley.edu/undergraduate/degree-programs/logic/#abouttheprogramtext">Logic</a>.
                My current research is with <a href="https://people.eecs.berkeley.edu/~svlevine/">Prof. Sergey Levine</a> at the <a href="https://rail.eecs.berkeley.edu/">Robotic AI and Learning Lab</a>.
                I am interested in building <a href="https://general-navigation-models.github.io/">generalizable and robust</a> robot learning systems that <a href="https://sites.google.com/view/SACSoN-review">continuously improve</a> with experience.
                Previously, I worked with <a href="https://web.engr.oregonstate.edu/~tgd/">Prof. Thomas Dietterich</a> on domain generalization techniques in computer vision.
              </p>
              <p>
                
                <!-- I am interested in the intersection of machine learning and robotics, particularly in the areas of reinforcement learning, imitation learning, and computer vision.
                I am also interested in the applications of these techniques to real-world problems. -->
              <p>
                
                
              </p>
              <p style="text-align:center">
                <a href="mailto:ajaysridhar@berkeley.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=wCRav7EAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/ajaysridhar0">Github</a> &nbsp/&nbsp
                <a href="files/ajay_cv_web.pdf">CV</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/ajay_web.png" class="hoverZoomLink">
              
              <!-- <a href="images/pfp_mountain 2.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/circle_pfp2.png" class="hoverZoomLink"></a> -->
            </td>
          </tr>
        </tbody>
      </table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Preprints</heading>
          <p>

          </p>
        </td>
      </tr>
    </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='nomad_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/nomad.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://general-navigation-models.github.io/nomad/index.html">
            <papertitle>NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration</papertitle>
          </a>
          <br>
          <strong>Ajay Sridhar</strong>,
          <a href="https://people.eecs.berkeley.edu/~shah/">Dhruv Shah</a>,
          <a href="https://www.linkedin.com/in/catherineglossop/">Catherine Glossop</a>,
          <a href="http://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
          <br>
          <em>arXiv, 2023</em>
          <br>
          <em>CoRL 2023 Workshop on Pre-Training for Robot Learning, 2023 <strong>(Oral Presentation)</strong></em>
          <br>
          <em>NeurIPS 2023 Workshop on Foundation Models for Decision Making, 2023 <strong>(Oral Presentation)</strong></em>
          <br>
          <a href="https://arxiv.org/abs/2310.07896">arXiv</a> /
          <a href="https://www.youtube.com/watch?v=zH8LaIapF6w&ab_channel=RAIL">Summary Video</a> /
          <a href="https://github.com/PrieureDeSion/visualnav-transformer">Code</a> /
          <p>
            NoMaD is a novel architecture for robotic navigation in previously unseen environments that uses a unified diffusion policy to jointly represent exploratory task-agnostic behavior and goal-directed task-specific behavior. 
          </p>
        </td>
      </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <p>

              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr onmouseout="vint_stop()" onmouseover="vint_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='vint_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/vint_arxiv.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>

        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://general-navigation-models.github.io/vint/index.html">
          <papertitle>ViNT: A Foundation Model for Visual Navigation</papertitle>
        </a>
        <br>
        <a href="https://people.eecs.berkeley.edu/~shah/">Dhruv Shah*</a>,
        <strong>Ajay Sridhar*</strong>,
        <a href="https://dashora7.github.io/">Nitish Dashora*</a>,
        <a href="https://kylesta.ch/">Kyle Stachowicz</a>,
        <a href="https://kevin.black/">Kevin Black</a>,
        <a href="https://sites.google.com/view/noriaki-hirose/">Noriaki Hirose</a>,
        <a href="http://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
        <br>
        <em>Conference on Robot Learning (CoRL), 2023 <strong>(Oral Presentation & Live Demonstration)</strong></em>
        <br>
        <em>Bay Area Machine Learning Symposium (BayLearn), 2023 <strong>(Oral Presentation)</strong></em>
        <br>
        <a href="https://arxiv.org/abs/2306.14846">arXiv</a> /
        <a href="https://www.youtube.com/watch?v=6kNex5dJ5sQ">Summary Video</a> /
        <a href="https://github.com/PrieureDeSion/visualnav-transformer">Code</a>
        <p>ViNT is a flexible Transformer-based model for visual navigation that can be efficiently adapated to a variety of downstream navigational tasks.</p>
      </td>
    </tr>

    

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='sacson_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/sacson.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://sites.google.com/view/SACSoN-review">
            <papertitle>SACSoN: Scalable Autonomous Control for Social Navigation</papertitle>
          </a>
          <br>
          <a href="https://sites.google.com/view/noriaki-hirose/">Noriaki Hirose</a>,
          <a href="https://people.eecs.berkeley.edu/~shah/">Dhruv Shah</a>,
          <strong>Ajay Sridhar</strong>,
          <a href="http://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
          <br>
          <em>IEEE Robotics and Automation Letters (RA-L), 2023</em>
          <br>
          <em>Conference on Robot Learning (CoRL), 2023 <strong>(Live Demonstration)</strong></em>
          <br>
          <a href="https://arxiv.org/abs/2306.01874">arXiv</a> /
          <a href="https://www.youtube.com/watch?v=AuYwmlUzi28">Summary Video</a> /
          <a href="https://sites.google.com/view/sacson-review/huron-dataset">Dataset</a>
          <p>SACSoN is vision-based navigation policy that learns socially unobtrusive behavior in human-occupied spaces through continual learning.</p>
        </td>
      </tr>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='gnm_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/gnm.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://sites.google.com/view/drive-any-robot">
            <papertitle>GNM: A General Navigation Model to Drive Any Robot</papertitle>
          </a>
          <br>
          <a href="https://people.eecs.berkeley.edu/~shah/">Dhruv Shah*</a>,
          <strong>Ajay Sridhar*</strong>,
          <a href="https://sites.google.com/view/noriaki-hirose/">Noriaki Hirose</a>,
          <a href="http://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
          <br>
          <em>International Conference on Robotics and Automation (ICRA), 2023</em>
          <br>
          <a href="https://arxiv.org/abs/2210.03370">arXiv</a> /
          <a href="https://youtu.be/ICeD6iOglKc">Summary Video</a> /
          <a href="https://github.com/PrieureDeSion/visualnav-transformer">Code</a> /
          <a href="https://www.marktechpost.com/2022/12/22/this-artificial-intelligence-ai-paper-from-uc-berkeley-presents-a-general-navigation-model-gnm-from-an-aggregated-multirobot-dataset-to-drive-any-robot/">Media Coverage</a>
          <p>GNM is vision-based navigation policy trained with a simple goal-reaching objective on a cross-embodiment navigation dataset. 
            It exhibits positive transfer, outperforming specialist models trained on singular embodiment datasets, and generalizes to new robots.
          </p>
        </td>
      </tr>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='exaug_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/exaug.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://sites.google.com/view/exaug-nav">
            <papertitle>ExAug: Robot-Conditioned Navigation Policies via Geometric Experience Augmentation</papertitle>
          </a>
          <br>
          <a href="https://sites.google.com/view/noriaki-hirose/">Noriaki Hirose</a>,
          <a href="https://people.eecs.berkeley.edu/~shah/">Dhruv Shah</a>,
          <strong>Ajay Sridhar</strong>,
          <a href="http://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
          <br>
          <em>International Conference on Robotics and Automation (ICRA), 2023</em>
          <br>
          <a href="https://arxiv.org/abs/2210.07450">arXiv</a> /
          <a href="https://www.youtube.com/watch?v=zWGpaxa1DKY">Summary Video</a> 
          <p>
            ExAug is a vision-based navigation policy that learns to control robots with varying camera types, camera placements, robot sizes, and velocity constraints by applying a novel geometric-aware objective to view augmented data.
          </p>
        </td>
      </tr>

    
    

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/cs188_bot.png" alt="cs188", width="100%">
          </td>
          <td width="75%" valign="center">
            <a href="https://inst.eecs.berkeley.edu/~cs188/fa23/">Undergraduate Student Instructor, CS188 Fall 2023</a>
            <br>
            <a href="https://inst.eecs.berkeley.edu/~cs188/sp23/">Undergraduate Student Instructor, CS188 Spring 2023</a>
            <br>
            <a href="https://inst.eecs.berkeley.edu/~cs188/fa23/">Undergraduate Student Instructor, CS188 Fall 2023</a>
            <br>
            <a href="https://inst.eecs.berkeley.edu/~cs188/sp22/">Undergraduate Student Instructor, CS188 Spring 2022</a>
          </td>
        </tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/berkeleyEECS.png" alt="berkeleyEECS" width="100%">
          </td>
          <td width="75%" valign="center">
            <a href="https://inst.eecs.berkeley.edu/~eecs16b/fa21/">Tutor, EECS16B Fall 2021</a>
            <br>
          </td>
        </tr>
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Source code from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
